---
aliases:
  - SRE
tags:
  - 문헌메모
  - 완료
---


---


Source : 
Author : 
URL :

## 시스템에 문제가 생긴다면?
- 당황하지 않아야 한다
- 침착하게 해결책을 찾는다
- 너무 부담스럽다고 느껴진다면 다른 이에게 도움을 청하거나 회사 전체를 호출해도 된다

## 테스트로 인한 장애
분산 MySQL 데이터베이스 중 하나에 포함된 테스트 데이터베이스를 통해 숨겨진 의존성을 확인하고자 수백 대의 데이터베이스 중 한 데이터베이스에 대한 접근을 차단하려고 하였다.
### 문제상황
테스트를 시작하고 몇 분 뒤 데이터베이스에 의존하는 여러 시스템으로부터 내부 및 외부 사용자가 핵심 시스템에 접근하지 못한다는 문제가 발생

### 대응
1. 테스트에 대한 후속 조치가 가능할 것이라고 판단해 테스트를 중단했지만 실패
2. SRE팀 전체가 모여 어떻게 복구해야 할 지 논의
3. 이미 테스트가 완료된 복제 및 장애 대비 데이터베이스의 권한을 복구
4. 개발자들에게 데이터베이스 애플리케이션 계층 라이브러리의 결함을 수정하도록 요청

복구 방법을 결정한 후 약 1시간이 채 지나기 전에 복구되었고, 이 테스트로 인해 영향을 받는 부분이 매우 광범위했으므로 라이브러리에 대한 신속한 수정이 이루어졌다. 또한 재발 바지를 위해 정기적 테스트 수행 계획도 수립되었다.

### 발견한 것들
- 잘한 부분
	- 장애에 영향을 받은 모든 서비스들이 즉각적으로 문제 보고
	- 여러 팁들이 자신들의 시스템 설정을 변개해 테스트 데이터베이스를 우회하는 등 여러 노력으로 서비스를 최대한 빨리 복구할 수 있었다
- 깨달은 사실
	- 해당 테스트는 철저한 리뷰를 거쳐 범위를 잘 규정했다고 생각했지만 실제로 의존 관계에 있는 시스템들 간의 상호 작용에 대해 완전히 이해하지 못하고 있던 것이 드러났다
	- 검증되지 않은 테스트는 모든 서비스와 고객이 장애 경험을 하게 된다는 점을 알게되었다
	- 테스트 환경에서의 롤백 절차를 완벽하게 테스트해야 하는 것을 알게되었다

---
## 변경으로 인한 장애
서비스의 악의적인 사용을 방지하기 위한 목적으로 인프라의 설정을 변경함
### 문제상황
해당 서비스는 외부로 노출되어 여러 서비스들과 함께 사용되던 서비스 였어서, 연관되어 있는 서비스들이 사용할 수 없는 상황이 됨

### 대응
 1. 첫 설정이 적용된 이후 5분쯤 지나, 해당 설정을 적용한 엔지니어도 사내 장애를 인지하고 롤백하기 위해 다시 설정을 변경해 서비스들이 점점 복구되었다.
 2. 처음 설정이 적용된 이후 10분쯤 지났을 무렵, 비상 대기 엔지니어가 장애를 선언하고 장애 조치 절차를 따르기 시작하였다.
 3. 설정을 적용했던 엔지니어는 비상 대기 엔지니어에게 장애의 원인이 자신이 적용한 설정 변경 때문인 것 같으며, 현재 해당 설정이 롤백된 상태라고 알렸다.

### 발견한 것들
- 잘한 부분
	- 모니터링 시스템은 거의 즉시 문제를 발견하고 알림을 발송하였지만, 알림이 스팸처럼 지속적으로 발생되어 온전하게 대응하지 못하였다.
	- 문제를 인지한 후의 장애 관리는 대체적으로 잘 진행됐고, 그 조치 과정 또한 신속하고 명확하게 전달됐다.
	- 장애가 발생한 도구 외에도 업데이트를 수행하거나 설정 변경을 롤백할 수 있는 도구와 대체 접근 방식을 보유하고 있어 장애 시간 동안 원활히 동작했고, 이번 장애를 통해 대체 도구들을 더 숙지하고 정기적으로 테스트하는 계기가 됨
	- 신속한 롤백 조치를 함

## 절차에 의한 장애
통상적인 자동화 테스트를 핟너 도중 곧 사용이 종료될 동일한 서버에 두 개의 종료 요청이 연속적으로 전달되었다.
### 문제상황
두 번째 시스템 종료 요청에 자동화 시스템의 사소한 버그로 전체 인프라스트럭처에서 동일한 설정으로 설치된 모든 서버에 대한 디스크삭제 요청이 큐에 기록되어 이 모든 서버들의 하드 드라이브가 곧 삭제될 위기에 처했다.

### 대응
1. 두 번째 시스템 종료 요청이 발송되자마자, 비상 대기 엔지니어에게 첫 번째 작은 서버가 오프라인 모드로 전환되었다는 호출이 전달되었다.
2. 조사 결과 해당 머신은 디스크삭제 큐의 요청에 때문에 종료된 것으로 파악되어 통상적인 절차에 따라 해당 지역의 트래픽을 다른 지역으로 우회시켰다.
3. 얼마 지나지 않아 동일한 설정을 가진 모든 서버들이 전세계에서 다운되기 시작했다. 비상 대기 엔지니어는 추가 피해가 발생하지 않도록 모든 팀의 자동화 시스템을 비활성화 했다.
4. 한 시간 정도 지나자 모든 트래픽이 다른 지역으로 이동되어 장애는 공식적으로 조치가 완료되었다.

### 발견한 것들
- 잘한 부분
	- 비상 대기 엔지니어가 신속하게 트래픽을 소규모 서버에서 대규모 서버로 이동시키는데 성공하였다.
	- 사고 발생 이후 사고 대응 프로세스를 신속하게 따라 문제를 해결하였다.

---

## 반복하지 않기
### 장애에 대한 기록을 남기자
```
무언가를 배우는 데 있어 과거에 있었던 일을 문서로 남기는 것보다 나은 방법은 없다.
```
- 장애에 대해 기록하고 모든 사람들이 장애를 통해 학습한 내용을 함께 공유할 수 있도록 한다.
- 과거에 발생한 장애로부터 학습한 내용들에 대한 기록을 갖추었ㄷ면 나중에 발생할 장애를 방지하기 위해 무엇을 해야 할지 알 수 있을 것이다.

## 사전 테스트 장려하기
```
실제로 장애가 발생하기 전까지는 시스템과 그 시스템에 의존적인 다른 시스템, 그리고 사용자가 어떻게 반응할지는 아무도 모른다.
```
- 실제로 장애가 발하기 전까지는 시스템과 그 시스템에 의존적인 다른 시스템, 그리고 사용자가 어떻게 반응할지 아무도 모르기 때문에 테스트한 적이 없는 방법에 절대 의존해서는 안 된다.

## 결론
- 우선 침착해야 한다.
- 필요하다면 다른 이에게 도움을 청해야 한다.
- 새로운 형태의 장애가 발생할 때마다 그 조치를 취한 사람은 문서화를 함께 해야한다.
시스템에 이런 과정을 지속적으로 도입하면, 발생한 장애 혹은 테스트 결과에 의해 절차와 시스템을 모두 개선할 수 있다고 한다.

### Link of Thoughts
Area : #300-DevOps/360-Site-Reliability-Engineering 

Keywords :
- [[SRE]]

Related Notes : 
- 